{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2NHWS99O8ctj2kuP67R4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthPanchi-256/RL_Project/blob/main/Q_Learning_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "leX_suAt_MlX"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize environment\n",
        "cliffEnv = gym.make(\"CliffWalking-v0\")\n",
        "\n",
        "# Initialize Q-table with small random values to prevent a cold start\n",
        "q_table = np.random.uniform(low=-1, high=1, size=(48, 4))\n",
        "\n",
        "# Epsilon-greedy policy function\n",
        "def policy(state, epsilon=0.0):\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(4)  # Random action (exploration)\n",
        "    return np.argmax(q_table[state])  # Best action (exploitation)\n",
        "\n",
        "# Hyperparameters\n",
        "EPSILON = 1.0  # Start with full exploration\n",
        "EPSILON_MIN = 0.01\n",
        "EPSILON_DECAY = 0.995  # Reduce exploration over time\n",
        "ALPHA = 0.1\n",
        "GAMMA = 0.9\n",
        "NUMBER_EPISODES = 100000\n",
        "\n",
        "# Training loop\n",
        "for episode in range(NUMBER_EPISODES):\n",
        "    state = cliffEnv.reset()  # Reset environment (new Gym API returns (state, info))\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    episode_length = 0\n",
        "\n",
        "    while not done:\n",
        "        action = policy(state, EPSILON)\n",
        "        next_state, reward, done, _ = cliffEnv.step(action)\n",
        "\n",
        "        # Q-learning update rule\n",
        "        q_table[state, action] += ALPHA * (reward + GAMMA * np.max(q_table[next_state]) - q_table[state, action])\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        episode_length += 1\n",
        "\n",
        "\n",
        "    # Decay epsilon\n",
        "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
        "\n",
        "    # Print progress\n",
        "    if episode % 1000 == 0:  # Print every 1000 episodes for readability\n",
        "        print(f\"Episode {episode}: Total reward: {total_reward}, Length: {episode_length}, Epsilon: {EPSILON:.3f}\")\n",
        "\n",
        "cliffEnv.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUbOTQ9a_Zdf",
        "outputId": "af2061dd-192a-4d99-a91f-16470a0bc238"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total reward: -172537, Length: 17206, Epsilon: 0.995\n",
            "Episode 1000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 2000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 3000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 4000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 5000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 6000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 7000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 8000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 9000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 10000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 11000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 12000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 13000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 14000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 15000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 16000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 17000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 18000: Total reward: -14, Length: 14, Epsilon: 0.010\n",
            "Episode 19000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 20000: Total reward: -15, Length: 15, Epsilon: 0.010\n",
            "Episode 21000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 22000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 23000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 24000: Total reward: -15, Length: 15, Epsilon: 0.010\n",
            "Episode 25000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 26000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 27000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 28000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 29000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 30000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 31000: Total reward: -15, Length: 15, Epsilon: 0.010\n",
            "Episode 32000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 33000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 34000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 35000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 36000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 37000: Total reward: -15, Length: 15, Epsilon: 0.010\n",
            "Episode 38000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 39000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 40000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 41000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 42000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 43000: Total reward: -17, Length: 17, Epsilon: 0.010\n",
            "Episode 44000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 45000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 46000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 47000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 48000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 49000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 50000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 51000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 52000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 53000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 54000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 55000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 56000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 57000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 58000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 59000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 60000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 61000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 62000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 63000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 64000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 65000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 66000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 67000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 68000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 69000: Total reward: -14, Length: 14, Epsilon: 0.010\n",
            "Episode 70000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 71000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 72000: Total reward: -123, Length: 24, Epsilon: 0.010\n",
            "Episode 73000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 74000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 75000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 76000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 77000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 78000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 79000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 80000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 81000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 82000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 83000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 84000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 85000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 86000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 87000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 88000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 89000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 90000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 91000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 92000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 93000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 94000: Total reward: -14, Length: 14, Epsilon: 0.010\n",
            "Episode 95000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 96000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 97000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 98000: Total reward: -13, Length: 13, Epsilon: 0.010\n",
            "Episode 99000: Total reward: -13, Length: 13, Epsilon: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(q_table, open(\"q_learning_q_table.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "D9iPQ8vEHhpt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cliffEnv = gym.make(\"CliffWalking-v0\")\n",
        "q_table = pickle.load(open(\"q_learning_q_table.pkl\",\"rb\"))\n",
        "\n",
        "done = False\n",
        "state = cliffEnv.reset()\n",
        "episode_length = 0\n",
        "total_reward = 0\n",
        "#episode loop\n",
        "while not done:\n",
        "  action = policy(state)\n",
        "  next_state,reward,done,info = cliffEnv.step(action)\n",
        "  print(\"action:\",action,\"state:\",state)\n",
        "  state = next_state\n",
        "  total_reward += reward\n",
        "  episode_length += 1\n",
        "  cliffEnv.render()\n",
        "print(\"Total reward:\",total_reward,\"episode length:\",episode_length)\n",
        "cliffEnv.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFsSNY7gH3oF",
        "outputId": "ca949b08-a438-4e32-ab69-0aeda212be81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action: 0 state: 36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action: 1 state: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action: 1 state: 25\n",
            "action: 1 state: 26\n",
            "action: 1 state: 27\n",
            "action: 1 state: 28\n",
            "action: 1 state: 29\n",
            "action: 1 state: 30\n",
            "action: 1 state: 31\n",
            "action: 1 state: 32\n",
            "action: 1 state: 33\n",
            "action: 1 state: 34\n",
            "action: 2 state: 35\n",
            "Total reward: -13 episode length: 13\n"
          ]
        }
      ]
    }
  ]
}